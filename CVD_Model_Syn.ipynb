{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61cce1c4-aeb4-429a-902f-801459921b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 907\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 23\n",
      "[LightGBM] [Info] Start training from score -0.600614\n",
      "[LightGBM] [Info] Start training from score -1.198156\n",
      "[LightGBM] [Info] Start training from score -1.898621\n",
      "\n",
      "=== Model Comparison (single train/test split) ===\n",
      "               Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "CatBoost          0.943      0.938   0.937     0.938    0.992\n",
      "XGBoost           0.940      0.934   0.934     0.934    0.991\n",
      "LogReg            0.939      0.928   0.940     0.934    0.991\n",
      "HistGB            0.940      0.934   0.933     0.934    0.991\n",
      "GradientBoost     0.940      0.935   0.931     0.933    0.991\n",
      "LightGBM          0.940      0.935   0.931     0.933    0.991\n",
      "ExtraTrees        0.940      0.940   0.927     0.933    0.991\n",
      "SVC-RBF           0.939      0.928   0.938     0.933    0.991\n",
      "GaussianNB        0.938      0.933   0.932     0.932    0.990\n",
      "LinearSVC         0.939      0.932   0.933     0.931      NaN\n",
      "SGD-Logistic      0.939      0.931   0.933     0.931    0.983\n",
      "RandomForest      0.934      0.931   0.920     0.925    0.989\n",
      "KNN               0.923      0.929   0.898     0.912    0.985\n",
      "DecisionTree      0.865      0.849   0.849     0.849    0.887\n",
      "AdaBoost          0.820      0.841   0.730     0.759    0.927\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "#  Cardiac Risk â€“ Model Benchmark  (single train/test split)\n",
    "# ==============================================================\n",
    "\n",
    "import warnings, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.metrics         import (accuracy_score, precision_score,\n",
    "                                     recall_score, f1_score, roc_auc_score)\n",
    "from sklearn.linear_model     import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes      import GaussianNB\n",
    "from sklearn.neighbors        import KNeighborsClassifier\n",
    "from sklearn.tree             import DecisionTreeClassifier\n",
    "from sklearn.svm              import SVC, LinearSVC\n",
    "from sklearn.ensemble         import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                                      GradientBoostingClassifier, AdaBoostClassifier,\n",
    "                                      HistGradientBoostingClassifier)\n",
    "import xgboost  as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost   import CatBoostClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  FILES  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "DATA_PATH = Path(\"D:\\generated_health_risk_data_100k.csv\")   # training CSV\n",
    "TEST_PATH = None                                # hold-out CSV or None\n",
    "LABEL_COL = \"Cardiac Risk\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  LOAD + BASIC CLEAN  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "df = (pd.read_csv(DATA_PATH, encoding=\"utf-8\")\n",
    "        .rename(columns=str.strip))\n",
    "\n",
    "test_df = (pd.read_csv(TEST_PATH, encoding=\"utf-8\").rename(columns=str.strip)\n",
    "           if TEST_PATH else None)\n",
    "\n",
    "if LABEL_COL not in df.columns:\n",
    "    raise ValueError(f\"'{LABEL_COL}' not in file. Headers: {list(df.columns)}\")\n",
    "\n",
    "def clean(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = d.copy()\n",
    "    d.drop(columns=[c for c in d.columns if \"id\" in c.lower()], inplace=True)\n",
    "    if \"Gender\" in d.columns:\n",
    "        d[\"Gender\"] = (d[\"Gender\"].astype(str).str.lower().str.strip()\n",
    "                       .map({\"male\":1,\"m\":1,\"female\":0,\"f\":0}))\n",
    "    return d\n",
    "\n",
    "df       = clean(df)\n",
    "if test_df is not None:\n",
    "    test_df = clean(test_df)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  TRAIN / TEST SPLIT  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "if test_df is None:\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.20, stratify=df[LABEL_COL].str.lower(), random_state=42)\n",
    "else:\n",
    "    train_df = df\n",
    "\n",
    "y_train = train_df[LABEL_COL].str.lower().map({\"low\":0,\"moderate\":1,\"high\":2})\n",
    "y_test  =  test_df[LABEL_COL].str.lower().map({\"low\":0,\"moderate\":1,\"high\":2})\n",
    "\n",
    "X_train = train_df.drop(columns=[LABEL_COL])\n",
    "X_test  = test_df.drop(columns=[LABEL_COL])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  PREPROCESSOR  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "num_cols = X_train.select_dtypes(\"number\").columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  MODEL ZOO (14 classifiers)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "models = {\n",
    "    \"LogReg\"        : LogisticRegression(max_iter=1500, class_weight=\"balanced\"),\n",
    "    \"GaussianNB\"    : GaussianNB(),\n",
    "    \"KNN\"           : KNeighborsClassifier(n_neighbors=15),\n",
    "    \"DecisionTree\"  : DecisionTreeClassifier(random_state=42),\n",
    "    \"LinearSVC\"     : LinearSVC(class_weight=\"balanced\"),\n",
    "    \"SVC-RBF\"       : SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\"),\n",
    "    \"SGD-Logistic\"  : SGDClassifier(loss=\"log_loss\", max_iter=2000,\n",
    "                                    class_weight=\"balanced\"),\n",
    "    \"RandomForest\"  : RandomForestClassifier(n_estimators=400, max_depth=18,\n",
    "                                             class_weight=\"balanced\", random_state=42),\n",
    "    \"ExtraTrees\"    : ExtraTreesClassifier(n_estimators=400, random_state=42),\n",
    "    \"GradientBoost\" : GradientBoostingClassifier(n_estimators=400, learning_rate=0.06),\n",
    "    \"AdaBoost\"      : AdaBoostClassifier(n_estimators=400, learning_rate=0.03),\n",
    "    \"HistGB\"        : HistGradientBoostingClassifier(max_iter=400),\n",
    "    \"XGBoost\"       : xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\",\n",
    "                                        max_depth=6, subsample=0.85,\n",
    "                                        colsample_bytree=0.9, random_state=42),\n",
    "    \"LightGBM\"      : lgb.LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\"      : CatBoostClassifier(verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  FIT & EVALUATE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([(\"prep\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    try:\n",
    "        y_prob = pipe.predict_proba(X_test)\n",
    "        auc    = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\":  accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"Recall\":    recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"F1-Score\":  f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"ROC-AUC\":   auc\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7.  RESULTS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "df_res = (pd.DataFrame(results).T\n",
    "          .sort_values(\"F1-Score\", ascending=False)\n",
    "          .round(3))\n",
    "\n",
    "print(\"\\n=== Model Comparison (single train/test split) ===\")\n",
    "print(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e82a860-f5e0-4709-a631-19aff8dfa7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§  Scrambled single-feature perfect predictors: []\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 2-bis.  Brute detection: single-feature perfect predictors\n",
    "# --------------------------------------------------------------------------\n",
    "perfect_cols = []\n",
    "for col in df.drop(columns=[LABEL]).columns:\n",
    "    # simple 80/20 split to test leakiness\n",
    "    train_idx = rng.choice(df.index, size=int(0.8*len(df)), replace=False)\n",
    "    test_idx  = df.index.difference(train_idx)\n",
    "    y_train   = y.loc[train_idx]\n",
    "    y_test    = y.loc[test_idx]\n",
    "\n",
    "    x_train = df.loc[train_idx, col]\n",
    "    x_test  = df.loc[test_idx,  col]\n",
    "\n",
    "    # If object, use mode mapping; if numeric, round/truncate\n",
    "    mapper = x_train.groupby(y_train).agg(lambda s: s.mode().iloc[0])\n",
    "    preds  = x_test.map({v:k for k,v in mapper.items()}).fillna(-1)\n",
    "\n",
    "    if (preds == y_test).all():\n",
    "        perfect_cols.append(col)\n",
    "\n",
    "for col in perfect_cols:\n",
    "    df[col] = rng.permutation(df[col].values)\n",
    "\n",
    "print(\"ðŸ”§  Scrambled single-feature perfect predictors:\", perfect_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7121a3f1-2aef-4ee5-854b-27460ee9cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exact numeric duplicates of the label: []\n"
     ]
    }
   ],
   "source": [
    "# -- Probe B: any numeric column that matches the target codes 0/1/2 -------\n",
    "dup_cols = []\n",
    "for c in X_enc.columns:\n",
    "    if set(X_enc[c].unique()) <= {0,1,2} and (X_enc[c] == y_enc).all():\n",
    "        dup_cols.append(c)\n",
    "print(\"\\nExact numeric duplicates of the label:\", dup_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5463d-5de3-4e10-a1f7-a3a2df0218ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
