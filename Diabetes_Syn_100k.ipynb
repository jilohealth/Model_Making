{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26b17b7-613c-4823-991a-d3bbdba873c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 745\n",
      "[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score -3.478078\n",
      "[LightGBM] [Info] Start training from score -1.140101\n",
      "[LightGBM] [Info] Start training from score -0.431789\n",
      "\n",
      "=== Diabetic Risk – Model Comparison ===\n",
      "                      Accuracy  Precision  Recall  F1-Score  ROC AUC\n",
      "CatBoost                 1.000      1.000   1.000     1.000    1.000\n",
      "HistGradientBoosting     0.999      0.999   0.996     0.997    1.000\n",
      "XGBoost                  0.998      0.998   0.984     0.991    1.000\n",
      "LightGBM                 0.997      0.997   0.978     0.987    1.000\n",
      "RandomForest             0.955      0.957   0.932     0.944    0.995\n",
      "GradientBoosting         0.986      0.983   0.855     0.900    0.999\n",
      "SVC-RBF                  0.935      0.865   0.944     0.899    0.992\n",
      "LogisticRegression       0.879      0.756   0.890     0.802    0.971\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Load and prepare data\n",
    "# ------------------------------------------------------------------\n",
    "DATA_PATH = Path(\"D:\\synthetic_diabetes_data_100000.csv\")\n",
    "LABEL     = \"DiabeticRisk\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Map target to numeric\n",
    "df[LABEL] = df[LABEL].str.lower().map({\"low\": 0, \"moderate\": 1, \"high\": 2})\n",
    "\n",
    "# Map gender to numeric\n",
    "df[\"Gender\"] = df[\"Gender\"].str.lower().map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "# Drop ID column\n",
    "df.drop(columns=[\"PatientID\"], inplace=True)\n",
    "\n",
    "# Split into train/test\n",
    "X = df.drop(columns=[LABEL])\n",
    "y = df[LABEL]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Preprocessing pipeline\n",
    "# ------------------------------------------------------------------\n",
    "num_cols = X_train.select_dtypes(include=\"number\").columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Fit & transform training data\n",
    "X_train_p = preprocessor.fit_transform(X_train)\n",
    "# Convert to dense if sparse\n",
    "if sparse.issparse(X_train_p):\n",
    "    X_train_p = X_train_p.toarray()\n",
    "\n",
    "# Transform test data\n",
    "X_test_p = preprocessor.transform(X_test)\n",
    "if sparse.issparse(X_test_p):\n",
    "    X_test_p = X_test_p.toarray()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Define models\n",
    "# ------------------------------------------------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=1000, class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=18,\n",
    "        class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.08, random_state=42\n",
    "    ),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_iter=300, random_state=42\n",
    "    ),\n",
    "    \"SVC-RBF\": SVC(\n",
    "        kernel=\"rbf\", probability=True,\n",
    "        class_weight=\"balanced\", random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        max_depth=6, subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Train & evaluate\n",
    "# ------------------------------------------------------------------\n",
    "metrics = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_p, y_train)\n",
    "    y_pred = model.predict(X_test_p)\n",
    "    # Attempt to get probabilities for AUC\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test_p)\n",
    "        auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    metrics[name] = {\n",
    "        \"Accuracy\":  accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"Recall\":    recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"F1-Score\":  f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"ROC AUC\":   auc\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Display results\n",
    "# ------------------------------------------------------------------\n",
    "results = (\n",
    "    pd.DataFrame(metrics)\n",
    "      .T\n",
    "      .sort_values(\"F1-Score\", ascending=False)\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Diabetic Risk – Model Comparison ===\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071445d-6067-432a-991d-7d8fba1dd454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
